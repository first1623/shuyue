#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ€§èƒ½æµ‹è¯•æŠ¥å‘Šç”Ÿæˆå™¨
è‡ªåŠ¨è¿è¡Œæ‰€æœ‰æ€§èƒ½æµ‹è¯•å¹¶ç”ŸæˆæŠ¥å‘Š
"""

import sys
import io

# è®¾ç½®æ ‡å‡†è¾“å‡ºç¼–ç ä¸ºUTF-8
sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', errors='replace')

import pytest
import time
import json
import os
from datetime import datetime
from typing import Dict, List


class PerformanceReport:
    """æ€§èƒ½æµ‹è¯•æŠ¥å‘Š"""
    
    def __init__(self):
        self.test_results = []
        self.start_time = None
        self.end_time = None
    
    def add_result(self, test_name: str, category: str, metrics: Dict):
        """æ·»åŠ æµ‹è¯•ç»“æœ"""
        self.test_results.append({
            "test_name": test_name,
            "category": category,
            "metrics": metrics,
            "timestamp": datetime.now().isoformat()
        })
    
    def generate_report(self) -> Dict:
        """ç”Ÿæˆå®Œæ•´æŠ¥å‘Š"""
        return {
            "report_title": "çŸ¥è¯†å›¾è°±ç³»ç»Ÿæ€§èƒ½æµ‹è¯•æŠ¥å‘Š",
            "test_date": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "duration": (self.end_time - self.start_time) if self.end_time and self.start_time else 0,
            "total_tests": len(self.test_results),
            "summary": self._generate_summary(),
            "details": self.test_results
        }
    
    def _generate_summary(self) -> Dict:
        """ç”Ÿæˆæ‘˜è¦"""
        categories = {}
        
        for result in self.test_results:
            category = result["category"]
            if category not in categories:
                categories[category] = {
                    "total": 0,
                    "passed": 0,
                    "failed": 0
                }
            
            categories[category]["total"] += 1
            
            # åˆ¤æ–­æµ‹è¯•æ˜¯å¦é€šè¿‡
            metrics = result["metrics"]
            if "success_rate" in metrics:
                if metrics["success_rate"] >= 95:
                    categories[category]["passed"] += 1
                else:
                    categories[category]["failed"] += 1
            elif "avg_ms" in metrics:
                # ç®€å•åˆ¤æ–­ï¼šå¹³å‡å“åº”æ—¶é—´å°äºç›®æ ‡å³é€šè¿‡
                target_ms = metrics.get("target_ms", 500)
                if metrics["avg_ms"] < target_ms:
                    categories[category]["passed"] += 1
                else:
                    categories[category]["failed"] += 1
        
        return categories
    
    def save_to_file(self, filename: str = "performance_report.json"):
        """ä¿å­˜æŠ¥å‘Šåˆ°æ–‡ä»¶"""
        report = self.generate_report()
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        print(f"âœ… æ€§èƒ½æµ‹è¯•æŠ¥å‘Šå·²ä¿å­˜åˆ°: {filename}")


def run_performance_tests():
    """è¿è¡Œæ‰€æœ‰æ€§èƒ½æµ‹è¯•"""
    # è·å–å½“å‰è„šæœ¬æ‰€åœ¨ç›®å½•
    script_dir = os.path.dirname(os.path.abspath(__file__))
    
    print("=" * 80)
    print("çŸ¥è¯†å›¾è°±ç³»ç»Ÿæ€§èƒ½åŸºå‡†æµ‹è¯•")
    print("=" * 80)
    
    report = PerformanceReport()
    report.start_time = time.time()
    
    # è¿è¡ŒAPIæ€§èƒ½æµ‹è¯•
    print("\nğŸ“Š è¿è¡ŒAPIæ€§èƒ½æµ‹è¯•...")
    exit_code = pytest.main([
        os.path.join(script_dir, "test_api_performance.py"),
        "-v",
        "-s",
        "--tb=short"
    ])
    
    # è¿è¡Œå¹¶å‘æµ‹è¯•
    print("\nğŸ”¥ è¿è¡Œå¹¶å‘å‹åŠ›æµ‹è¯•...")
    pytest.main([
        os.path.join(script_dir, "test_concurrent.py"),
        "-v",
        "-s",
        "--tb=short"
    ])
    
    # è¿è¡Œæ•°æ®åº“æ€§èƒ½æµ‹è¯•
    print("\nğŸ—„ï¸ è¿è¡Œæ•°æ®åº“æ€§èƒ½æµ‹è¯•...")
    pytest.main([
        os.path.join(script_dir, "test_database_performance.py"),
        "-v",
        "-s",
        "--tb=short"
    ])
    
    report.end_time = time.time()
    
    # ç”ŸæˆæŠ¥å‘Š
    report_json = report.generate_report()
    
    print("\n" + "=" * 80)
    print("æ€§èƒ½æµ‹è¯•æ‘˜è¦")
    print("=" * 80)
    print(f"æµ‹è¯•æ—¶é—´: {report_json['test_date']}")
    print(f"æ€»æµ‹è¯•æ•°: {report_json['total_tests']}")
    print(f"æµ‹è¯•è€—æ—¶: {report_json['duration']:.2f}ç§’")
    
    report_path = os.path.join(script_dir, "performance_report.json")
    report.save_to_file(report_path)
    
    return report_json


if __name__ == "__main__":
    run_performance_tests()
