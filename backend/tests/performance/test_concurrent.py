#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¹¶å‘å‹åŠ›æµ‹è¯•
æµ‹è¯•ç›®æ ‡ï¼š
1. æ”¯æŒ100å¹¶å‘ç”¨æˆ·
2. æ”¯æŒ500å¹¶å‘å³°å€¼
3. ç³»ç»Ÿç¨³å®šæ€§æµ‹è¯•
"""

import sys
import os

# æ·»åŠ backendç›®å½•åˆ°Pythonè·¯å¾„
backend_dir = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
if backend_dir not in sys.path:
    sys.path.insert(0, backend_dir)

import pytest
import time
import threading
import statistics
from typing import List, Dict
from fastapi.testclient import TestClient

try:
    from main import app
    client = TestClient(app)
except ImportError:
    # å¦‚æœmainæ¨¡å—ä¸å¯ç”¨ï¼Œåˆ›å»ºä¸€ä¸ªæ¨¡æ‹Ÿçš„æµ‹è¯•
    app = None
    client = None


class ConcurrentTestResult:
    """å¹¶å‘æµ‹è¯•ç»“æœ"""
    
    def __init__(self):
        self.response_times: List[float] = []
        self.status_codes: List[int] = []
        self.errors: List[str] = []
        self.lock = threading.Lock()
    
    def record_success(self, response_time: float, status_code: int):
        """è®°å½•æˆåŠŸè¯·æ±‚"""
        with self.lock:
            self.response_times.append(response_time)
            self.status_codes.append(status_code)
    
    def record_error(self, error: str):
        """è®°å½•é”™è¯¯"""
        with self.lock:
            self.errors.append(error)
    
    def get_stats(self) -> Dict:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        if not self.response_times:
            return {
                "total_requests": 0,
                "errors": len(self.errors),
                "error_rate": 100.0
            }
        
        return {
            "total_requests": len(self.response_times) + len(self.errors),
            "successful_requests": len(self.response_times),
            "failed_requests": len(self.errors),
            "success_rate": len(self.response_times) / (len(self.response_times) + len(self.errors)) * 100,
            "avg_ms": statistics.mean(self.response_times),
            "min_ms": min(self.response_times),
            "max_ms": max(self.response_times),
            "median_ms": statistics.median(self.response_times),
            "p95_ms": statistics.quantiles(self.response_times, n=100)[94] if len(self.response_times) >= 20 else max(self.response_times),
        }


def make_concurrent_requests(
    endpoint: str,
    num_users: int,
    requests_per_user: int = 10
) -> ConcurrentTestResult:
    """
    å¹¶å‘è¯·æ±‚æµ‹è¯•
    
    Args:
        endpoint: APIç«¯ç‚¹
        num_users: å¹¶å‘ç”¨æˆ·æ•°
        requests_per_user: æ¯ä¸ªç”¨æˆ·çš„è¯·æ±‚æ•°
    
    Returns:
        æµ‹è¯•ç»“æœ
    """
    result = ConcurrentTestResult()
    
    def user_task():
        """å•ä¸ªç”¨æˆ·çš„è¯·æ±‚ä»»åŠ¡"""
        for _ in range(requests_per_user):
            try:
                start_time = time.time()
                response = client.get(endpoint)
                duration = (time.time() - start_time) * 1000
                
                result.record_success(duration, response.status_code)
                
            except Exception as e:
                result.record_error(str(e))
    
    # åˆ›å»ºçº¿ç¨‹
    threads = [threading.Thread(target=user_task) for _ in range(num_users)]
    
    # å¯åŠ¨æ‰€æœ‰çº¿ç¨‹
    start_time = time.time()
    for thread in threads:
        thread.start()
    
    # ç­‰å¾…æ‰€æœ‰çº¿ç¨‹å®Œæˆ
    for thread in threads:
        thread.join()
    
    total_duration = time.time() - start_time
    
    return result, total_duration


class TestConcurrentPerformance:
    """å¹¶å‘æ€§èƒ½æµ‹è¯•"""
    
    @pytest.mark.skipif(client is None, reason="FastAPI app not available")
    def test_10_concurrent_users(self):
        """æµ‹è¯•10ä¸ªå¹¶å‘ç”¨æˆ·"""
        print("\nğŸ”¥ æµ‹è¯•10ä¸ªå¹¶å‘ç”¨æˆ·...")
        
        result, duration = make_concurrent_requests(
            "/api/v1/graph/stats",
            num_users=10,
            requests_per_user=5
        )
        
        stats = result.get_stats()
        
        print(f"âœ… æ€»è¯·æ±‚æ•°: {stats['total_requests']}")
        print(f"âœ… æˆåŠŸè¯·æ±‚æ•°: {stats['successful_requests']}")
        print(f"âœ… å¤±è´¥è¯·æ±‚æ•°: {stats['failed_requests']}")
        print(f"âœ… æˆåŠŸç‡: {stats['success_rate']:.2f}%")
        print(f"âœ… å¹³å‡å“åº”æ—¶é—´: {stats['avg_ms']:.2f}ms")
        print(f"âœ… P95å“åº”æ—¶é—´: {stats['p95_ms']:.2f}ms")
        print(f"âœ… æ€»è€—æ—¶: {duration:.2f}ç§’")
        
        # éªŒæ”¶æ ‡å‡†
        assert stats['success_rate'] >= 99, f"âŒ æˆåŠŸç‡è¿‡ä½: {stats['success_rate']:.2f}%"
        assert stats['avg_ms'] < 500, f"âŒ å¹³å‡å“åº”æ—¶é—´è¿‡é•¿: {stats['avg_ms']:.2f}ms"
    
    @pytest.mark.skipif(client is None, reason="FastAPI app not available")
    def test_50_concurrent_users(self):
        """æµ‹è¯•50ä¸ªå¹¶å‘ç”¨æˆ·"""
        print("\nğŸ”¥ æµ‹è¯•50ä¸ªå¹¶å‘ç”¨æˆ·...")
        
        result, duration = make_concurrent_requests(
            "/api/v1/graph/stats",
            num_users=50,
            requests_per_user=5
        )
        
        stats = result.get_stats()
        
        print(f"âœ… æ€»è¯·æ±‚æ•°: {stats['total_requests']}")
        print(f"âœ… æˆåŠŸç‡: {stats['success_rate']:.2f}%")
        print(f"âœ… å¹³å‡å“åº”æ—¶é—´: {stats['avg_ms']:.2f}ms")
        print(f"âœ… P95å“åº”æ—¶é—´: {stats['p95_ms']:.2f}ms")
        print(f"âœ… æ€»è€—æ—¶: {duration:.2f}ç§’")
        
        # éªŒæ”¶æ ‡å‡†
        assert stats['success_rate'] >= 98, f"âŒ æˆåŠŸç‡è¿‡ä½: {stats['success_rate']:.2f}%"
        assert stats['avg_ms'] < 1000, f"âŒ å¹³å‡å“åº”æ—¶é—´è¿‡é•¿: {stats['avg_ms']:.2f}ms"
    
    @pytest.mark.skipif(client is None, reason="FastAPI app not available")
    def test_100_concurrent_users(self):
        """æµ‹è¯•100ä¸ªå¹¶å‘ç”¨æˆ·"""
        print("\nğŸ”¥ æµ‹è¯•100ä¸ªå¹¶å‘ç”¨æˆ·...")
        
        result, duration = make_concurrent_requests(
            "/api/v1/graph/stats",
            num_users=100,
            requests_per_user=3
        )
        
        stats = result.get_stats()
        
        print(f"âœ… æ€»è¯·æ±‚æ•°: {stats['total_requests']}")
        print(f"âœ… æˆåŠŸç‡: {stats['success_rate']:.2f}%")
        print(f"âœ… å¹³å‡å“åº”æ—¶é—´: {stats['avg_ms']:.2f}ms")
        print(f"âœ… P95å“åº”æ—¶é—´: {stats['p95_ms']:.2f}ms")
        print(f"âœ… æ€»è€—æ—¶: {duration:.2f}ç§’")
        print(f"âœ… ååé‡: {stats['total_requests'] / duration:.2f} req/s")
        
        # éªŒæ”¶æ ‡å‡†
        assert stats['success_rate'] >= 95, f"âŒ æˆåŠŸç‡è¿‡ä½: {stats['success_rate']:.2f}%"
        assert stats['avg_ms'] < 1500, f"âŒ å¹³å‡å“åº”æ—¶é—´è¿‡é•¿: {stats['avg_ms']:.2f}ms"
    
    @pytest.mark.skipif(client is None, reason="FastAPI app not available")
    def test_500_concurrent_users_stress(self):
        """æµ‹è¯•500ä¸ªå¹¶å‘ç”¨æˆ·ï¼ˆå‹åŠ›æµ‹è¯•ï¼‰"""
        print("\nğŸ”¥ğŸ”¥ğŸ”¥ æµ‹è¯•500ä¸ªå¹¶å‘ç”¨æˆ·ï¼ˆå‹åŠ›æµ‹è¯•ï¼‰...")
        
        result, duration = make_concurrent_requests(
            "/api/v1/graph/stats",
            num_users=500,
            requests_per_user=1
        )
        
        stats = result.get_stats()
        
        print(f"âœ… æ€»è¯·æ±‚æ•°: {stats['total_requests']}")
        print(f"âœ… æˆåŠŸè¯·æ±‚æ•°: {stats['successful_requests']}")
        print(f"âœ… å¤±è´¥è¯·æ±‚æ•°: {stats['failed_requests']}")
        print(f"âœ… æˆåŠŸç‡: {stats['success_rate']:.2f}%")
        print(f"âœ… å¹³å‡å“åº”æ—¶é—´: {stats['avg_ms']:.2f}ms")
        print(f"âœ… æ€»è€—æ—¶: {duration:.2f}ç§’")
        print(f"âœ… ååé‡: {stats['total_requests'] / duration:.2f} req/s")
        
        # å‹åŠ›æµ‹è¯•çš„éªŒæ”¶æ ‡å‡†ç›¸å¯¹å®½æ¾
        assert stats['success_rate'] >= 90, f"âŒ æˆåŠŸç‡è¿‡ä½: {stats['success_rate']:.2f}%"
        assert stats['avg_ms'] < 3000, f"âŒ å¹³å‡å“åº”æ—¶é—´è¿‡é•¿: {stats['avg_ms']:.2f}ms"
    
    @pytest.mark.skipif(client is None, reason="FastAPI app not available")
    def test_mixed_endpoints_concurrent(self):
        """æµ‹è¯•æ··åˆAPIç«¯ç‚¹å¹¶å‘è®¿é—®"""
        print("\nğŸ”¥ æµ‹è¯•æ··åˆAPIç«¯ç‚¹å¹¶å‘è®¿é—®...")
        
        endpoints = [
            "/api/v1/graph/stats",
            "/api/v1/knowledge-tree/statistics",
            "/api/v1/graph/data",
        ]
        
        result = ConcurrentTestResult()
        
        def user_task(endpoint: str):
            """å•ç”¨æˆ·ä»»åŠ¡"""
            try:
                start_time = time.time()
                response = client.get(endpoint)
                duration = (time.time() - start_time) * 1000
                result.record_success(duration, response.status_code)
            except Exception as e:
                result.record_error(str(e))
        
        threads = []
        for endpoint in endpoints:
            for _ in range(20):  # æ¯ä¸ªç«¯ç‚¹20ä¸ªå¹¶å‘ç”¨æˆ·
                thread = threading.Thread(target=user_task, args=(endpoint,))
                threads.append(thread)
        
        start_time = time.time()
        for thread in threads:
            thread.start()
        
        for thread in threads:
            thread.join()
        
        duration = time.time() - start_time
        stats = result.get_stats()
        
        print(f"âœ… æ€»è¯·æ±‚æ•°: {stats['total_requests']}")
        print(f"âœ… æˆåŠŸç‡: {stats['success_rate']:.2f}%")
        print(f"âœ… å¹³å‡å“åº”æ—¶é—´: {stats['avg_ms']:.2f}ms")
        print(f"âœ… æ€»è€—æ—¶: {duration:.2f}ç§’")
        print(f"âœ… ååé‡: {stats['total_requests'] / duration:.2f} req/s")
        
        assert stats['success_rate'] >= 98, f"âŒ æˆåŠŸç‡è¿‡ä½: {stats['success_rate']:.2f}%"


if __name__ == "__main__":
    pytest.main([__file__, "-v", "-s"])
